pb <- pbeta(0.75)
pb <- pbeta(0.75,1)
pb <- pbeta(0.75,2,1)
pb
?pbinom
pbinom(7,8,0.5)
pbinom(6,8,0.5)
pbinom(7,8,0.5,lowr.tail=FALSE)
pbinom(7,8,0.5,lower.tail=FALSE)
pbinom(6,8,0.5,lower.tail=FALSE)
pbinom(5,8,0.5,lower.tail=FALSE)
pbinom(8,8,0.5,lower.tail=FALSE)
pbinom(7,8,0.5,lower.tail=FALSE)
qnorm(0.95)
ppois(40,9*5)
?sample
sample(0:1,1000)
sample(0:1,1000,replace=TRUE)
sample(0:1,1000,replace=TRUE)/(1:n)
sample(0:1,1000,replace=TRUE)/(1:1000)
cumsum(sample(0:1,1000,replace=TRUE))/(1:1000)
plot(cumsum(sample(0:1,1000,replace=TRUE))/(1:1000))
library(UsingR)
download.packages(UsingR)
download.packages("UsingR")
download.packages(date)
download.packages("date"")
download.packages("date")
setwd("~/")
download.packages("date")
file.info(x)
file.info(
)
install.packages(c("labeling", "psych"))
download.packages("date")
?destdir
?destdir()
?file.info(x)
download.packages("date.R")
download.packages("date.R",destdir=C:\Users\user\AppData\Local\Temp\RtmporU4ma\downloaded_packages)
download.packages("date.R",destdir="C:\Users\user\AppData\Local\Temp\RtmporU4ma\downloaded_packages")
download.packages(date.R,destdir="C:\Users\user\AppData\Local\Temp\RtmporU4ma\downloaded_packages")
download.packages("date.R",destdir="C:/Users\user\AppData/Local/Temp/RtmporU4ma/downloaded_packages")
download.packages("date.R",destdir="/Users\user\AppData/Local/Temp/RtmporU4ma/downloaded_packages")
download.packages("date.R",destdir=.)
download.packages("date.R",destdir=NULL)
download.packages(date.R,destdir=NULL)
download.packages(date.R)
library("ggplot2", lib.loc="~/R/win-library/3.1")
install.packages("date")
install.packages(UsingR)
install.packages("UsingR")
data(fater.son)
library(UsingR)
data(father.son)
View(father.son)
summary(father.son)
x <- father.son$sheight
mean(x)
mean(x)+c(-1,1)*qnorm(0.975)*sd(x)/sqrt(length(x))
mean(x)+c(-1,1)*qnorm(0.95)*sd(x)/sqrt(length(x))
x <- father.son$fheight
mean(x)+c(-1,1)*qnorm(0.95)*sd(x)/sqrt(length(x))
poisson.test(10)
poisson.test(10)$conf
poisson.test(10,T = 1)$conf
poisson.test(60,T = 1)$conf
poisson.test(10,T = 60)$conf
poisson.test(10,T = 1)$conf
poisson.test(0.1,T = 1)$conf
poisson.test(6,T = 1)$conf
poisson.test(10,T = 10)$conf
poisson.test(10,T = 60)$conf
poisson.test(10,T = 6)$conf
poisson.test(10,T = 3)$conf
poisson.test(10,T = 2)$conf
poisson.test(10,T = 1)$conf
poisson.test(20,T = 1)$conf
poisson.test(5,T = 1)$conf
install.packages(ISLR)
install.packages("ISLR")
library("ggplot2", lib.loc="~/R/win-library/3.1")
library(ISLR); library(ggplot2)
data(Wage)
inTrain <- createDataPartition(y=Wage$wage,p=0.7,list=F) #create 75% train set of data
trainSet <- spam[inTrain,]
testSet <- spam[-inTrain,]
trainSet <- Wage[inTrain,]
testSet <- Wage[-inTrain,]
inTrain <- createDataPartition(y=Wage$wage,p=0.7,list=F) #create 75% train set of data
library("caret") # a ML models package
inTrain <- createDataPartition(y=Wage$wage,p=0.7,list=F) #create 75% train set of data
trainSet <- Wage[inTrain,]
testSet <- Wage[-inTrain,]
featurePlot(x=trainSet[,c("age","jobclass")],y=trainSet$wage,plot="pairs") #present the wage date vs. age & jobclass
qplot(wage,color=eductaion,data=trainSet,geom="density") # a density plot of wage vs. education
qplot(wage,color=education,data=trainSet,geom="density") # a density plot of wage vs. education
library(kernlab); library("e1071"); data(spam)
library(kernlab); library("e1071"); data(spam)
inTrain <- createDataPartition(y=spam$type,p=0.75,list=F) #create 75% train set of data
trainSet <- spam[inTrain,]
testSet <- spam[-inTrain,]
folds1 <- createFolds(y=trainSet$type,k=10,list=TRUE,returnTrain = TRUE)#create a 10 kfold cross validation on train set for features optimization
folds2 <- createResample(y=spam$type,times=10,list=TRUE)#create a 10 kfold resampling on data with replacement
#sapply(folds1,length) #size of folds
modelFit <- train(type ~ .,data=trainSet,method="glm") #train a glm algo on data
modelFit$finalModel #best fitted model
predictions <- predict(modelFit,newdata=testSet) #run the fitted model on test data
confusionMatrix(predictions,testSet$type) #creates a hypotesis tableconfusion matrix
#Standardizing the features
preObj <- preProcess(trainSet[,-58],method==c("center","scale"))
trainSetStandart <- predict(preObj,trainSet[,-58])$capitalAAve
preObj <- preProcess(trainSet[,-58],method=c("center","scale"))
trainSetStandart <- predict(preObj,trainSet[,-58])$capitalAAve
preObj <- preProcess(trainSet[,-58],method=c("center","scale","knnImpute"))
inTrain <- createDataPartition(y=Wage$wage,p=0.7,list=F) #create 75% train set of data
trainSet <- Wage[inTrain,]
testSet <- Wage[-inTrain,]
dummies <- dummyVars(wage ~ jobclass,data=trainSet)
head(dummies)
head(dummies,newdata=trainSet)
head(predict(dummies,newdata=trainSet))
nsv <- nearZeroVar(trainSet,saveMetrics = T) #check for zero covariates
nsv
#Initialization
{
sessionInfo() #system performance
gc() #clear unsued memory
rm(list=ls()) # clear workspace
mainDir <- "C:/Users/user/Documents"
setwd(mainDir)
set.seed(1234) #setting seed for comparison
memory.limit(size=3000)
library(h2o); library(e1071)
}
data <- read.csv("Book1.csv",header = F)
View(data)
hist(data$V2)
hist(data$V1)
hist(data$V3)
hist(data$V1,breaks = 30)
hist(data$V3,breaks = 10)
hist(data$V2,breaks = 12)
hist(data$V2,breaks = 13)
data <- read.csv("Book1.csv",header = F)
View(data)
data <- read.csv("Book1.csv",header = F)
View(data)
data <- read.csv("Book1.csv",header = T)
View(data)
hist(data$name)
table(data$month)
table(data$day)
table(data$name)
View(data)
table(sort(data$name))
hist(data$month,breaks = 12)
install.packages("readr")
library(readr)
setwd("C:/Users/user/Documents/R/CrowdFlower")
train <- read_csv("train.csv")
test  <- read_csv("test.csv")
summary(train)
View(train)
View(test)
M <- ncol(train)
N <- nrow(train)
Ntest <- nrow(test)
unique(train$query)[1:10]
# The number of unique train queries
length(unique(train$query))
# The number of unique test queries
length(unique(test$query))
# Are any queries different between the sets?
length(setdiff(unique(train$query), unique(test$query)))
length(unique(train$product_title))
# The number of product titles that are only in the train set or only in the test set
length(setdiff(unique(train$product_title), unique(test$product_title)))
# The number of product titles that are in both the train and test sets
length(intersect(unique(train$product_title), unique(test$product_title)))
install.packages(c("ggvis","tm"))
#Some basic text analysis on the queries
library(ggvis)
library(tm)
plot_word_counts <- function(documents) {
# Keep only unique documents and convert them to lowercase
corpus <- Corpus(VectorSource(tolower(unique(documents))))
# Remove punctuation from the documents
corpus <- tm_map(corpus, removePunctuation)
# Remove english stopwords, such as "the" and "a"
corpus <- tm_map(corpus, removeWords, stopwords("english"))
doc_terms <- DocumentTermMatrix(corpus)
doc_terms <- as.data.frame(as.matrix(doc_terms))
word_counts <- data.frame(Words=colnames(doc_terms), Counts=colSums(doc_terms))
# Sort from the most frequent words to the least frequent words
word_counts <- word_counts[order(word_counts$Counts, decreasing=TRUE),]
top_words <- word_counts[1:10,]
top_words$Words <- factor(top_words$Words, levels=top_words$Words)
# Plot the 10 most frequent words with ggvis
top_words %>%
ggvis(~Words, ~Counts) %>%
layer_bars(fill:="#20beff")
}
plot_word_counts(c(train$query, test$query))
set.seed(0)
plot_word_counts(sample(c(train$product_title, test$product_title), 1000))
library(readr)
library(tm)
library(wordcloud)
install.packages("wordcloud")
library(wordcloud)
make_word_cloud <- function(documents) {
corpus = Corpus(VectorSource(tolower(documents)))
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords("english"))
frequencies = DocumentTermMatrix(corpus)
word_frequencies = as.data.frame(as.matrix(frequencies))
words <- colnames(word_frequencies)
freq <- colSums(word_frequencies)
wordcloud(words, freq,
min.freq=sort(freq, decreasing=TRUE)[[100]],
colors=brewer.pal(8, "Dark2"),
random.color=TRUE)
}
library(dplyr)
set.seed(0)
make_word_cloud(train$query)
libs<-c("readr","SnowballC","tm","wordcloud")
lapply(libs,require,character.only=TRUE)
install.packages("SnowballC")
libs<-c("readr","SnowballC","tm","wordcloud")
lapply(libs,require,character.only=TRUE)
#Loading Your working directory here
query<-VCorpus(VectorSource(train$query))#Selects only query variable
query
#Term-Document Matrix
tdm<-TermDocumentMatrix(query,control=list(tolower=TRUE,removePunctuation=TRUE,
removeNumbers=TRUE,stopwords=TRUE,
stemming=TRUE))
tdm<-as.matrix(tdm)
word_freq<-sort(rowSums(tdm),decreasing=TRUE)#Obtains word frequencies
df<-data.frame(word=names(word_freq), freq=word_freq)#Builds a data-frame
#Wordcloud
png("wordcloud.png", width=12, height=8, units="in", res=300)
wordcloud(df$word, df$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
dev.off()
View(df)
library(Metrics)
library(tm)
library(SnowballC)
library(e1071)
library(Matrix)
library(SparseM)
library(readr)
library(readr)
library(Metrics)
library(tm)
library(SnowballC)
library(e1071)
library(Matrix)
library(SparseM)
library(caTools)
View(df)
#Initialization
{
sessionInfo() #system performance
gc() #clear unsued memory
rm(list=ls()) # clear workspace
mainDir <- "C:/Users/user/Documents/R/CrowdFlower"
setwd(mainDir)
Sys.setlocale("LC_TIME", "English")
set.seed(66719543) #setting seed for comparison
library(fpp); library(UsingR)
library(caret); library(kernlab)
library(foreach); library(doParallel)
library(ggplot2); library(h2o)
library(randomForest); library(date)
library(data.table); library(bit64)
library(matrixStats); library(e1071)
library(FSelector); library(lubridate)
library(nnet); library(quantmod)
library(neuralnet); library(minpack.lm)
library(RSNNS); library(ROCR)
library(Metrics); library(tm)
library(SnowballC); library(e1071)
library(Matrix); library(SparseM)
library(readr); library(ggvis)
library(tm); library(wordcloud)
}
#Initialization
{
sessionInfo() #system performance
gc() #clear unsued memory
rm(list=ls()) # clear workspace
mainDir <- "C:/Users/user/Documents/R/CrowdFlower"
setwd(mainDir)
Sys.setlocale("LC_TIME", "English")
set.seed(66719543) #setting seed for comparison
library(fpp); library(UsingR)
library(caret); library(kernlab)
library(foreach); library(doParallel)
library(ggplot2); library(h2o)
library(randomForest); library(date)
library(data.table); library(bit64)
library(matrixStats); library(e1071)
library(FSelector); library(lubridate)
library(nnet); library(quantmod)
library(neuralnet); library(minpack.lm)
library(RSNNS); library(ROCR)
library(Metrics); library(tm)
library(SnowballC); library(e1071)
library(Matrix); library(SparseM)
library(readr); library(ggvis)
library(tm); library(wordcloud)
}
#Read Data
{
train <- read.csv("train.csv",header=T)
test <- read.csv("test.csv",header=T)
sampleSubmission <- read.csv("sampleSubmission.csv",header=T)
M <- ncol(train)
N <- nrow(train)
Ntest <- nrow(test)
}
sampleSubmission <- read.csv("sampleSubmission.csv",header=T)
M <- ncol(train)
N <- nrow(train)
Ntest <- nrow(test)
train <- read_csv("train.csv",header=T)
train <- read_csv("train.csv")
test <- read_csv("test.csv")
View(sampleSubmission)
length(unique(train$query))
# The number of unique test queries
length(unique(test$query))
# Are any queries different between the sets?
length(setdiff(unique(train$query), unique(test$query)))
plot_word_counts <- function(documents) {
plot_word_counts <- function(documents) {
# Creating a helper function plot_word_counts to plot counts of word occurences in different sets
plot_word_counts <- function(documents) {
# Keep only unique documents and convert them to lowercase
corpus <- Corpus(VectorSource(tolower(unique(documents))))
# Remove punctuation from the documents
corpus <- tm_map(corpus, removePunctuation)
# Remove english stopwords, such as "the" and "a"
corpus <- tm_map(corpus, removeWords, stopwords("english"))
doc_terms <- DocumentTermMatrix(corpus)
doc_terms <- as.data.frame(as.matrix(doc_terms))
word_counts <- data.frame(Words=colnames(doc_terms), Counts=colSums(doc_terms))
# Sort from the most frequent words to the least frequent words
word_counts <- word_counts[order(word_counts$Counts, decreasing=TRUE),]
top_words <- word_counts[1:10,]
top_words$Words <- factor(top_words$Words, levels=top_words$Words)
# Plot the 10 most frequent words with ggvis
top_words %>%
ggvis(~Words, ~Counts) %>%
layer_bars(fill:="#20beff")
}
plot_word_counts(c(train$query, test$query))
top_words_query <- plot_word_counts(c(train$query, test$query))
top_words_query$data
top_words_query$cur_data
#Word count in query
query <- VCorpus(VectorSource(train$query)) #Selects only query variable
#Term-Document Matrix
tdm<-TermDocumentMatrix(query,control=list(tolower=TRUE,removePunctuation=TRUE,
removeNumbers=TRUE,stopwords=TRUE,stemming=TRUE))
tdm<-as.matrix(tdm)
word_freq<-sort(rowSums(tdm),decreasing=TRUE)#Obtains word frequencies
df<-data.frame(word=names(word_freq), freq=word_freq)#Builds a data-frame
query$1
View(tdm)
View(df)
query_word_freq_df <- data.frame(word=names(word_freq), freq=word_freq,row.names = F)#Builds a data-frame
query_word_freq_df <- data.frame(word=names(word_freq), freq=word_freq)#Builds a data-frame
View(df)
View(query_word_freq_df)
rm_garbage = function(string){
garbage = c("<.*?>", "http", "www","img","border","style","px","margin","left", "right","font","solid","This translation tool is for your convenience only.*?Note: The accuracy and accessibility of the resulting translation is not guaranteed")
for (i in 1:length(garbage)){
string = gsub(garbage[i], "", string)
}
return (string)
}
train$product_description = lapply(train$product_description,rm_garbage)
train$median_relevance = as.factor(train$median_relevance)
View(train)
test$product_description = lapply(test$product_description,rm_garbage)
View(test)
?combi
train$query
train$query
train$query = as.factor(train$query)
train$query
levels(train$query)
test$query = factor(train$query,levels = levels(train$query))
plot_word_counts(c(train$query, test$query))
length(unique(train$query))
length(unique(test$query))
length(setdiff(unique(train$query), unique(test$query)))
test$query = factor(test$query,levels = levels(train$query))
summary(train)
summary(test)
str(test)
#Initialization
{
sessionInfo() #system performance
gc() #clear unsued memory
rm(list=ls()) # clear workspace
mainDir <- "C:/Users/user/Documents/R/CrowdFlower"
setwd(mainDir)
Sys.setlocale("LC_TIME", "English")
set.seed(66719543) #setting seed for comparison
library(fpp); library(UsingR)
library(caret); library(kernlab)
library(foreach); library(doParallel)
library(ggplot2); library(h2o)
library(randomForest); library(date)
library(data.table); library(bit64)
library(matrixStats); library(e1071)
library(FSelector); library(lubridate)
library(nnet); library(quantmod)
library(neuralnet); library(minpack.lm)
library(RSNNS); library(ROCR)
library(Metrics); library(tm)
library(SnowballC); library(e1071)
library(Matrix); library(SparseM)
library(readr); library(ggvis)
library(tm); library(wordcloud)
}
install.packages("Metrics")
library(Metrics); library(tm)
library(SnowballC); library(e1071)
library(Matrix); library(SparseM)
library(readr); library(ggvis)
library(tm); library(wordcloud)
#Read Data
{
train <- read_csv("train.csv")
test <- read_csv("test.csv")
sampleSubmission <- read.csv("sampleSubmission.csv",header=T)
M <- ncol(train)
N <- nrow(train)
Ntest <- nrow(test)
}
ids = test$id
rtrain = nrow(train)
rtest =nrow(test)
relevance = as.factor(train$median_relevance)
variance = train$relevance_variance
# We don't need you anymoreeee
train$median_relevance = NULL
train$relevance_variance = NULL
# Combine train and test set for the dragons
combi=rbind(train,test)
View(combi)
corpus <- Corpus(VectorSource(combi$query))
all_text <- Corpus(VectorSource(combi$query))
dtm<-DocumentTermMatrix(all_text,control=list(tolower=TRUE,removePunctuation=TRUE,
dtm<-DocumentTermMatrix(all_text,control=list(tolower=TRUE,removePunctuation=TRUE,
removeNumbers=TRUE,stopwords=TRUE,
stemming=TRUE,weighting=function(x) weightTfIdf(x,normalize=T)))
?Corpus
?DocumentTermMatrix
dtm <- removeSparseTerms(dtm,0.99)
df_q<-Matrix(as.matrix(dtm),sparse=T)
df_q<-as.data.frame(as.matrix(dtm))
colnames(df_q)=paste("q_",colnames(df_q),sep="")
View(df_q)
